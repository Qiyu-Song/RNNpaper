#!/bin/bash
#SBATCH -J UltimateRNN
##SBATCH -n 1
#SBATCH -o out-%j
#SBATCH -e eo-%j
#SBATCH -p gpu_requeue
#SBATCH --gres=gpu:4
#SBATCH --constraint="[a40|a100|v100|h100]"
#SBATCH -t 2-10:00     # job time in minutes
#SBATCH --mem-per-cpu=64000 #MB
##SBATCH --exclusive
##SBATCH --no-requeue
#SBATCH --exclude=holygpu8a[25104,22505,22405]
#SBATCH --mail-type=ALL
#SBATCH --mail-user=qsong@g.harvard.edu

export NCCL_DEBUG=INFO
export PYTHONFAULTHANDLER=1

module load python/3.10.12-fasrc01
mamba activate pt2.0.1_cuda11.8

# stage 1 or 2, train for h0
#lr=1e-4
#batchsize=256
#max_epoch=1000

# stage 1, all params
#lr=5e-6
#batchsize=128
#max_epoch=10000

# stage 2, all params
lr=2e-6
batchsize=256
max_epoch=5000

echo "Running with lr=$lr, batchsize=$batchsize, max_epoch=$max_epoch"
srun python ultimaternn_addp_includewave_lightning.py --lr=$lr --batchsize=$batchsize --max_epoch=$max_epoch

